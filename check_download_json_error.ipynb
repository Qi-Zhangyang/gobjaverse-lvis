{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if the download files contain errors or omissions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the errors and omissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "root_directory = 'The Download path.'\n",
    "errors_and_omissions_files = []\n",
    "\n",
    "# Iterate through the first-level directories from 0 to 159\n",
    "for i in range(160):\n",
    "    subdir_path = os.path.join(root_directory, str(i))\n",
    "    if os.path.exists(subdir_path) and os.path.isdir(subdir_path):\n",
    "        # Iterate through directories similar to \"10005\" as the second-level directories\n",
    "        for subdirectory in os.listdir(subdir_path):\n",
    "            subdirectory_path = os.path.join(subdir_path, subdirectory)\n",
    "            if os.path.isdir(subdirectory_path):\n",
    "                # Iterate through folders from \"00000\" to \"00039\"\n",
    "                for j in range(40):\n",
    "                    json_filename = f\"{j:05}/{j:05}.json\"\n",
    "                    json_filepath = os.path.join(subdirectory_path, json_filename)\n",
    "                    if os.path.exists(json_filepath):\n",
    "                        try:\n",
    "                            with open(json_filepath, 'r', encoding='utf-8') as json_file:\n",
    "                                json_data = json.load(json_file)\n",
    "                                if \"offset\" not in json_data:\n",
    "                                    # If the JSON file does not contain the \"offset\" field, add its path to the corrupted files list\n",
    "                                    errors_and_omissions_files.append(json_filepath)\n",
    "                        except Exception as e:\n",
    "                            # Catch JSON file parsing exceptions and add their paths to the corrupted files list\n",
    "                            errors_and_omissions_files.append(json_filepath)\n",
    "                    else:\n",
    "                        errors_and_omissions_files.append(json_filepath)\n",
    "\n",
    "# Store the list of these files as a JSON file\n",
    "with open('check_download_json_error/download_json_error.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(errors_and_omissions_files, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Re-download the errors and omissions files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('check_download_json_error/download_json_error.json', 'r') as json_file:\n",
    "    json_content = json.load(json_file)\n",
    "\n",
    "json_download_content = []\n",
    "for json_line in json_content:\n",
    "    json_line = json_line.split('/')[5] + '/' + json_line.split('/')[6]\n",
    "    json_download_content.append(json_line)\n",
    "\n",
    "json_download_content = list(set(json_download_content))\n",
    "\n",
    "with open('check_download_json_error/add_download.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(json_download_content, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you get the 'add_download.json', you can ru:\n",
    "python ./download_gobjaverse_280k.py PATH_TO_DOWNLOAD check_download_json_error/add_download.json 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Delete the errors and omissions files and change 'download_280k_lvis.json'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the errors and omissions files.\n",
    "\n",
    "with open('check_download_json_error/add_download.json', 'r') as json_file:\n",
    "    path_list = json.load(json_file)\n",
    "\n",
    "current_directory = 'The Download path.'\n",
    "\n",
    "for relative_path in path_list:\n",
    "    full_path = os.path.join(current_directory, relative_path) \n",
    "    try:\n",
    "        if os.path.exists(full_path) and os.path.isdir(full_path):\n",
    "            shutil.rmtree(full_path) \n",
    "    except Exception as e:\n",
    "        print(f\"Can not delete {full_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the download_280k_lvis.json\n",
    "\n",
    "with open('annotations/download_280k_lvis.json', 'r') as json_file:\n",
    "    ori_full_list = json.load(json_file)\n",
    "\n",
    "with open('check_download_json_error/add_download.json', 'r') as json_file:\n",
    "    error_list = json.load(json_file)\n",
    "\n",
    "filtered_list = [x for x in ori_full_list if x not in error_list]\n",
    "\n",
    "with open('annotations/download_280k_lvis_wo_error.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(filtered_list, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly download the no error json file: download_280k_lvis_wo_error.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# 'gobjaverse_280k_index_to_objaverse.json' is provided by the official.\n",
    "hf_hub_download(repo_id=\"alexzyqi/GPT4Point\", filename='download_280k_lvis_wo_error.json', repo_type=\"dataset\", local_dir='annotations/')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
